{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "469b721c",
   "metadata": {},
   "source": [
    "# Get SSAST embeddings\n",
    "Get embeddings from a fine-tuned SSAST model\n",
    "\n",
    "Author(s): Matt, Daniela Wiepert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e54e6f0",
   "metadata": {},
   "source": [
    "To begin, you will need access to google cloud storage bucket and the following packages must be installed on your system \n",
    "\n",
    "* opencv-python\n",
    "* albumentations (may run into issues in AIF)\n",
    "* librosa\n",
    "* torch, torchvision, torchaudio\n",
    "\n",
    "(can ignore the following if using AIF)\n",
    "* google-cloud\n",
    "* google-cloud-storage\n",
    "* google-cloud-bigquery\n",
    "\n",
    "If working on a local computer, you can run the following commands to gain access to the google storage bucket\n",
    "\n",
    "```gcloud auth application-default login```\n",
    "\n",
    "```gcloud auth application-defaul set-quota-project PROJECT_NAME```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebdebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "#built-in\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "\n",
    "#local\n",
    "from dataloader_mayo import AudioDataset\n",
    "from models import ASTModel_finetune\n",
    "from utilities.dataloader_utils import collate_fn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f31e1f31",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "There are a few arguments that must be set when getting embeddings, including listing the CSV containing files to get embeddings for, the prefix for where the audio files are located, GCS bucket/project names, and locations for the fine-tuned models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "#data loading\n",
    "parser.add_argument('-d','--data_csv', default='gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620/test.csv', help='path to data csv. Assumes it points to a csv file.')\n",
    "parser.add_argument('-i','--prefix',default='speech_ai/speech_lake/speech_poc_freeze_1', help='Input directory or location in google cloud storage bucket containing audio files to load')\n",
    "#GCS\n",
    "parser.add_argument('-b','--bucket_name', default='ml-e107-phi-shared-aif-us-p', help=\"google cloud storage bucket name\")\n",
    "parser.add_argument('-p','--project_name', default='ml-mps-aif-afdgpet01-p-6827', help='google cloud platform project name')\n",
    "#librosa vs torchaudio\n",
    "parser.add_argument('--lib', default=True, type=bool, help=\"Specify whether to load using librosa as compared to torch audio\")\n",
    "#output\n",
    "parser.add_argument('-o',\"--exp_dir\", type=str, default=\"/Users/m144443/Documents/GitHub/mayo-ssast/temp_out\", help=\"directory with model + associated files\")\n",
    "parser.add_argument('-mn',\"--model_name\", type=str, default=\"ast_mdl_base_mayo_13_adamw_1epoch.pt\", help=\"directory with model + associated files\" )\n",
    "#embedding batch size\n",
    "parser.add_argument('-bs',\"--batch_size\", type=int, default=8, help=\"batch size for embeddings\")\n",
    "parser.add_argument('--num_workers', default=0, type=int, metavar='NW', help='# of workers for dataloading (default: 32)')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8edd6ea1",
   "metadata": {},
   "source": [
    "## Setting up environment\n",
    "The first step is to make sure the GCS bucket is initialized if given a `bucket_name`. Additionally, the list of target labels must be set. \n",
    "\n",
    "In the original implementation, the list must be given as a `.txt` file to pass through the command line. In this implementation, we will set it as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5cfc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS set up\n",
    "if args.bucket_name is not None:\n",
    "    storage_client = storage.Client(project=args.project_name)\n",
    "    bq_client = bigquery.Client(project=args.project_name)\n",
    "    bucket = storage_client.bucket(args.bucket_name)\n",
    "else:\n",
    "    bucket = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target labels\n",
    "target_labels=['breathy',\n",
    "             'loudness decay',\n",
    "             'slow rate',\n",
    "             'high pitch',\n",
    "             'hoarse / harsh',\n",
    "             'irregular artic breakdowns',\n",
    "             'rapid rate',\n",
    "             'reduced OA loudness',\n",
    "             'abn pitch variability',\n",
    "             'strained',\n",
    "             'hypernasal',\n",
    "             'abn loudness variability',\n",
    "              'distortions']\n",
    "#set number of target classes for classification\n",
    "args.n_class = len(target_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a26cfc0",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "The data must be loaded in steps, starting by loading in the label data, then setting up audio configurations for evaluation, then generating an AudioDataset object, and finally setting up the dataloader.\n",
    "\n",
    "When loading data, we start with a full path to a `.csv` file, with file names  and the associated label data. We will get an embedding for each file.\n",
    "\n",
    "The audio configurations are dictionaries with parameters for altering the audio and generating spectrograms.\n",
    "\n",
    "The AudioDatasets are set up in the `dataloader_mayo.py` script, using transforms specified in `utilities/dataloader_utils.py`. \n",
    "\n",
    "Finally, the dataloader takes in the dataset and batch size + number of workers.\n",
    "\n",
    "Please note that the resulting samples will be a dictionary with the keys `uid`, `fbank`, `waveform`, `targets`, `sample_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6043b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and alter data as necessary\n",
    "test_df = pd.read_csv(args.data_csv, index_col = 'uid')\n",
    "test_df[\"distortions\"]=((test_df[\"distorted Cs\"]+test_df[\"distorted V\"])>0).astype(int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fa40ede",
   "metadata": {},
   "source": [
    "## Get embeddings\n",
    "The `get_ssast_embeddings` function takes in a dataframe, target_labels, the args list, and a GCS bucket\n",
    "\n",
    "The function first loads the original model arguments, sets up the dataset, loads the original model, then gets the embedding layer and calculates embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ssast_embeddings(df, target_labels, args, bucket):\n",
    "    # (1) To get embeddings, first load the arguments used for fine-tuning the model\n",
    "    args_path = os.path.join(args.exp_dir, 'args.pkl') \n",
    "    model_path = os.path.join(args.exp_dir, args.model_name)\n",
    "\n",
    "    # (2) set up audio conformer with original model args\n",
    "    with open(args_path, 'rb') as f:\n",
    "        model_args = pickle.load(f)\n",
    "    \n",
    "    #TEMP FIX\n",
    "    print('delete fix later')\n",
    "    #the model we debugged with was from a version without skip_norm, and when we were attempting pretraining. Needed to temporarily fix. \n",
    "    model_args.skip_norm = False\n",
    "    model_args.task = 'ft_cls'\n",
    "    model_args.pretrained_mdl_path = '/Users/m144443/Documents/GitHub/mayo-ssast/pretrained_model/SSAST-Base-Frame-400.pth'\n",
    "    #model_args.pretrained_mdl_path = '/Users/m144443/Documents/GitHub/mayo-ssast/temp_out/ast_mdl_base_mayo_13_adamw_1epoch.pt'\n",
    "\n",
    "    audio_conf = {'dataset': model_args.dataset, 'mode': 'evaluation', 'resample_rate': model_args.resample_rate, 'reduce': model_args.reduce, 'clip_length': model_args.clip_length,\n",
    "                    'tshift':model_args.tshift, 'speed':model_args.speed, 'gauss_noise':model_args.gauss, 'pshift':model_args.pshift, 'pshiftn':model_args.pshiftn, 'gain':model_args.gain, 'stretch': model_args.stretch,\n",
    "                    'num_mel_bins': model_args.num_mel_bins, 'target_length': model_args.target_length, 'freqm': model_args.freqm, 'timem': model_args.timem, 'mixup': model_args.mixup, 'noise':model_args.noise,\n",
    "                    'mean':model_args.dataset_mean, 'std':model_args.dataset_std, 'skip_norm':model_args.skip_norm}\n",
    "    \n",
    "    # (3) set up dataloader with current args\n",
    "    dataset = AudioDataset(annotations_df=df, target_labels=target_labels, audio_conf=audio_conf, \n",
    "                                prefix=args.prefix, bucket=bucket, librosa=args.lib)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True, collate_fn=collate_fn) \n",
    "    \n",
    "    # (4) load AST model with original model parameters + pre-trained/fine-tuned model\n",
    "    ast_mdl = ASTModel_finetune(task=model_args.task, label_dim=model_args.n_class, \n",
    "                                    fshape=model_args.fshape, tshape=model_args.tshape, \n",
    "                                    fstride=model_args.fstride, tstride=model_args.tstride,\n",
    "                                    input_fdim=model_args.num_mel_bins, input_tdim=model_args.target_length, \n",
    "                                    model_size=model_args.model_size, load_pretrained_mdl_path=model_args.pretrained_mdl_path)\n",
    " \n",
    "    ast_mdl.eval()\n",
    "    #load fine-tuned model\n",
    "    ast_mdl.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    # (5) get the embedding layer\n",
    "    activation = {}\n",
    "    def _get_activation(name):\n",
    "        def _hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return _hook\n",
    "    ast_mdl.mlp_head[0].register_forward_hook(_get_activation('embeddings'))\n",
    "    \n",
    "    # (6) Calculate embeddings\n",
    "    print('Calculating Embeddings')\n",
    "    all_names=[]\n",
    "    all_embeddings=[]\n",
    "    for batch in tqdm(loader):\n",
    "        logits=ast_mdl(batch['fbank'])\n",
    "        names = batch['uid']\n",
    "        #index = \n",
    "        all_names.append(names)\n",
    "        all_embeddings.append(activation['embeddings'].detach().numpy())\n",
    "        \n",
    "    all_names=np.concatenate(all_names)\n",
    "    all_embeddings=np.concatenate(all_embeddings)\n",
    "    \n",
    "    embedding_df=pd.DataFrame(all_embeddings).set_index(all_names)\n",
    "    embedding_df.columns=[str(s) for s in embedding_df.columns]\n",
    "    \n",
    "    return embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = get_ssast_embeddings(test_df, target_labels, args, bucket)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c249d4cd",
   "metadata": {},
   "source": [
    "The embedding dataframe will be saved in the same directory as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) save embeddings\n",
    "out_name = os.path.join(args.model_dir, 'embeddings.csv')\n",
    "embeddings_df.to_csv(out_name)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
