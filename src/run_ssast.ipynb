{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSAST for speech\n",
    "All functions copied from this repo and then edited as indicated\n",
    "\n",
    "https://github.com/YuanGongND/ssast\n",
    "Additional authors: Matt, Daniela\n",
    "\n",
    "Original ASTModel class was split into two - one for pretraining and one for finetuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, you will need access to google cloud storage bucket and the following packages must be installed on your system \n",
    "\n",
    "* opencv-python\n",
    "* albumentations (may run into issues in AIF)\n",
    "* librosa\n",
    "* torch, torchvision, torchaudio\n",
    "\n",
    "(can ignore the following if using AIF)\n",
    "* google-cloud\n",
    "* google-cloud-storage\n",
    "* google-cloud-bigquery\n",
    "\n",
    "If working on a local computer, you can run the following commands to gain access to the google storage bucket\n",
    "\n",
    "```gcloud auth application-default login```\n",
    "```gcloud auth application-defaul set-quota-project PROJECT_NAME```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from google.cloud import storage, bigquery\n",
    "\n",
    "from src.models.ast_models import ASTModel_pretrain, ASTModel_finetune\n",
    "from src.dataloader_gcs import AudioDataset\n",
    "from utilities.ssast_utils import *\n",
    "from utilities.speech_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load data from google storage bucket\n",
    "\n",
    "project_name = 'ml-mps-aif-afdgpet01-p-6827'\n",
    "study = 'speech_poc_freeze_1'\n",
    "bucket_name = 'ml-e107-phi-shared-aif-us-p'\n",
    "gcs_prefix = f'speech_ai/speech_lake/{study}'\n",
    "\n",
    "storage_client = storage.Client(project=project_name)\n",
    "bq_client = bigquery.Client(project=project_name)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "file_list=[]\n",
    "for blob in storage_client.list_blobs(bucket_name, prefix='speech_ai/speech_lake/speech_poc_freeze_1'):\n",
    "    file_list.append(blob.name)\n",
    "\n",
    "    extensions=[f.split('.')[-1] for f in file_list]\n",
    "\n",
    "data_split_root = 'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620'\n",
    "gcs_train_path = f'{data_split_root}/train.csv'\n",
    "gcs_test_path = f'{data_split_root}/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (1) load the train and test files to a df\n",
    "train_df = pd.read_csv(gcs_train_path, index_col = 'uid')\n",
    "test_df = pd.read_csv(gcs_test_path, index_col = 'uid')\n",
    "\n",
    "# (2) alter columns as necessary \n",
    "train_df[\"distortions\"]=((train_df[\"distorted Cs\"]+train_df[\"distorted V\"])>0).astype(int)\n",
    "test_df[\"distortions\"]=((test_df[\"distorted Cs\"]+test_df[\"distorted V\"])>0).astype(int)\n",
    "\n",
    "# (3) define target labels\n",
    "target_labels=['breathy',\n",
    "             'loudness decay',\n",
    "             'slow rate',\n",
    "             'high pitch',\n",
    "             'hoarse / harsh',\n",
    "             'irregular artic breakdowns',\n",
    "             'rapid rate',\n",
    "             'reduced OA loudness',\n",
    "             'abn pitch variability',\n",
    "             'strained',\n",
    "             'hypernasal',\n",
    "             'abn loudness variability',\n",
    "              'distortions']\n",
    "\n",
    "# (4) select only the target labels from train and test df\n",
    "train_df=train_df[target_labels]\n",
    "test_df=train_df[target_labels]\n",
    "\n",
    "# (5) prep the data\n",
    "prep_ssast_data(train_df,target_labels,'train_ssast',create_label_csv=True)\n",
    "prep_ssast_data(test_df,target_labels,'test_ssast')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SSAST\n",
    "additional imports to support running SSAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "basepath = os.path.dirname(os.path.dirname(sys.path[0]))\n",
    "sys.path.append(basepath)\n",
    "import dataloader\n",
    "from models import ASTModel_pretrain, ASTModel_finetune\n",
    "import numpy as np\n",
    "from traintest import train, validate\n",
    "from traintest_mask import trainmask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set arguments for running SSAST\n",
    "#TODO: label csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set arguments for running pre-training/fine-tuning\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--data-train\", type=str, default='train_ssast.json', help=\"training data json\")\n",
    "parser.add_argument(\"--data-val\", type=str, default=None, help=\"validation data json\")\n",
    "parser.add_argument(\"--data-eval\", type=str, default='test_ssast.json', help=\"evaluation data json\")\n",
    "parser.add_argument(\"--label-csv\", type=str, default='./label_df.csv', help=\"csv with class labels\")\n",
    "parser.add_argument(\"--n_class\", type=int, default=len(target_labels), help=\"number of classes\")\n",
    "\n",
    "parser.add_argument(\"--dataset\", type=str, default='demo', help=\"the dataset used for training\")\n",
    "parser.add_argument(\"--dataset_mean\", type=float, default= -4.2677393, help=\"the dataset mean, used for input normalization\")\n",
    "parser.add_argument(\"--dataset_std\", type=float, default=4.5689974, help=\"the dataset std, used for input normalization\")\n",
    "parser.add_argument(\"--target_length\", type=int, default=1024, help=\"the input length in frames\")\n",
    "parser.add_argument(\"--num_mel_bins\", type=int, default=128, help=\"number of input mel bins\")\n",
    "\n",
    "parser.add_argument(\"--exp-dir\", type=str, default=\"\", help=\"directory to dump experiments\")\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.0001, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--warmup', help='if use warmup learning rate scheduler', type=ast.literal_eval, default='True')\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"sgd\", \"adam\"])\n",
    "parser.add_argument('-b', '--batch-size', default=8, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('-w', '--num-workers', default=8, type=int, metavar='NW', help='# of workers for dataloading (default: 32)')\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=80, help=\"number of maximum training epochs\")\n",
    "# only used in pretraining stage or from-scratch fine-tuning experiments\n",
    "parser.add_argument(\"--lr_patience\", type=int, default=2, help=\"how many epoch to wait to reduce lr if mAP doesn't improve\")\n",
    "parser.add_argument('--adaptschedule', help='if use adaptive scheduler ', type=ast.literal_eval, default='False')\n",
    "\n",
    "parser.add_argument(\"--n-print-steps\", type=int, default=100, help=\"number of steps to print statistics\")\n",
    "parser.add_argument('--save_model', help='save the models or not', type=ast.literal_eval, default='True')\n",
    "\n",
    "parser.add_argument('--freqm', help='frequency mask max length', type=int, default=0)\n",
    "parser.add_argument('--timem', help='time mask max length', type=int, default=0)\n",
    "parser.add_argument(\"--mixup\", type=float, default=0, help=\"how many (0-1) samples need to be mixup during training\")\n",
    "parser.add_argument(\"--bal\", type=str, default=None, help=\"use balanced sampling or not\")\n",
    "# the stride used in patch spliting, e.g., for patch size 16*16, a stride of 16 means no overlapping, a stride of 10 means overlap of 6.\n",
    "# during self-supervised pretraining stage, no patch split overlapping is used (to aviod shortcuts), i.e., fstride=fshape and tstride=tshape\n",
    "# during fine-tuning, using patch split overlapping (i.e., smaller {f,t}stride than {f,t}shape) improves the performance.\n",
    "# it is OK to use different {f,t} stride in pretraining and finetuning stages (though fstride is better to keep the same)\n",
    "# but {f,t}stride in pretraining and finetuning stages must be consistent.\n",
    "parser.add_argument(\"--fstride\", type=int,default=128,help=\"soft split freq stride, overlap=patch_size-stride\")\n",
    "parser.add_argument(\"--tstride\", type=int,default=2, help=\"soft split time stride, overlap=patch_size-stride\")\n",
    "parser.add_argument(\"--fshape\", type=int, defaut=128, help=\"shape of patch on the frequency dimension\")\n",
    "parser.add_argument(\"--tshape\", type=int, default=2, help=\"shape of patch on the time dimension\")\n",
    "parser.add_argument('--model_size', help='the size of AST models', type=str, default='base')\n",
    "\n",
    "parser.add_argument(\"--task\", type=str, default='ft_cls', help=\"pretraining or fine-tuning task\", choices=[\"ft_avgtok\", \"ft_cls\", \"pretrain_mpc\", \"pretrain_mpg\", \"pretrain_joint\"])\n",
    "\n",
    "# pretraining augments\n",
    "#parser.add_argument('--pretrain_stage', help='True for self-supervised pretraining stage, False for fine-tuning stage', type=ast.literal_eval, default='False')\n",
    "parser.add_argument('--mask_patch', help='how many patches to mask (used only for ssl pretraining)', type=int, default=400)\n",
    "parser.add_argument(\"--cluster_factor\", type=int, default=3, help=\"mask clutering factor\")\n",
    "parser.add_argument(\"--epoch_iter\", type=int, default=2000, help=\"for pretraining, how many iterations to verify and save models\")\n",
    "\n",
    "# fine-tuning arguments\n",
    "parser.add_argument(\"--pretrained_mdl_path\", type=str, default='./pretrained_model/SSAST-Base-Frame-400.pth', help=\"the ssl pretrained models path\")\n",
    "parser.add_argument(\"--head_lr\", type=int, default=1, help=\"the factor of mlp-head_lr/lr, used in some fine-tuning experiments only\")\n",
    "parser.add_argument(\"--noise\", help='if augment noise in finetuning', type=ast.literal_eval, default='False')\n",
    "parser.add_argument(\"--metrics\", type=str, default=\"mAP\", help=\"the main evaluation metrics in finetuning\", choices=[\"mAP\", \"acc\"])\n",
    "parser.add_argument(\"--lrscheduler_start\", default=10, type=int, help=\"when to start decay in finetuning\")\n",
    "parser.add_argument(\"--lrscheduler_step\", default=5, type=int, help=\"the number of step to decrease the learning rate in finetuning\")\n",
    "parser.add_argument(\"--lrscheduler_decay\", default=0.5, type=float, help=\"the learning rate decay ratio in finetuning\")\n",
    "parser.add_argument(\"--wa\", help='if do weight averaging in finetuning', type=ast.literal_eval, default='False')\n",
    "parser.add_argument(\"--wa_start\", type=int, default=16, help=\"which epoch to start weight averaging in finetuning\")\n",
    "parser.add_argument(\"--wa_end\", type=int, default=30, help=\"which epoch to end weight averaging in finetuning\")\n",
    "parser.add_argument(\"--loss\", type=str, default=\"BCE\", help=\"the loss function for finetuning, depend on the task\", choices=[\"BCE\", \"CE\"])\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset spectrogram mean and std, used to normalize the input\n",
    "# norm_stats = {'librispeech':[-4.2677393, 4.5689974], 'howto100m':[-4.2677393, 4.5689974], 'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
    "# target_length = {'librispeech': 1024, 'howto100m':1024, 'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "# # if add noise for data augmentation, only use for speech commands\n",
    "# noise = {'librispeech': False, 'howto100m': False, 'audioset': False, 'esc50': False, 'speechcommands':True}\n",
    "\n",
    "audio_conf = {'num_mel_bins': args.num_mel_bins, 'target_length': args.target_length, 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup, 'dataset': args.dataset,\n",
    "              'mode':'train', 'mean':args.dataset_mean, 'std':args.dataset_std, 'noise':args.noise}\n",
    "\n",
    "val_audio_conf = {'num_mel_bins': args.num_mel_bins, 'target_length': args.target_length, 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': args.dataset,\n",
    "                  'mode': 'evaluation', 'mean': args.dataset_mean, 'std': args.dataset_std, 'noise': False}\n",
    "\n",
    "# if use balanced sampling, note - self-supervised pretraining should not use balance sampling as it implicitly leverages the label information.\n",
    "if args.bal == 'bal':\n",
    "    print('balanced sampler is being used')\n",
    "    samples_weight = np.loadtxt(args.data_train[:-5]+'_weight.csv', delimiter=',')\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf),\n",
    "        batch_size=args.batch_size, sampler=sampler, num_workers=args.num_workers, pin_memory=False, drop_last=True)\n",
    "else:\n",
    "    print('balanced sampler is not used')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf),\n",
    "        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=False, drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataloader.AudioDataset(args.data_val, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "    batch_size=args.batch_size * 2, shuffle=False, num_workers=args.num_workers, pin_memory=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Now train with {:s} with {:d} training samples, evaluate with {:d} samples'.format(args.dataset, len(train_loader.dataset), len(val_loader.dataset)))\n",
    "\n",
    "# in the pretraining stage\n",
    "if 'pretrain' in args.task:\n",
    "    cluster = (args.num_mel_bins != args.fshape)\n",
    "    if cluster == True:\n",
    "        print('The num_mel_bins {:d} and fshape {:d} are different, not masking a typical time frame, using cluster masking.'.format(args.num_mel_bins, args.fshape))\n",
    "    else:\n",
    "        print('The num_mel_bins {:d} and fshape {:d} are same, masking a typical time frame, not using cluster masking.'.format(args.num_mel_bins, args.fshape))\n",
    "    # no label dimension needed as it is self-supervised, fshape=fstride and tshape=tstride\n",
    "    audio_model = ASTModel_pretrain(fshape=args.fshape, tshape=args.tshape, fstride=args.fshape, tstride=args.tshape,\n",
    "                       input_fdim=args.num_mel_bins, input_tdim=args.target_length, model_size=args.model_size, load_pretrained_mdl_path=args.pretrained_mdl_pth)\n",
    "# in the fine-tuning stage\n",
    "else:\n",
    "    audio_model = ASTModel_finetune(task = args.finetune_task, label_dim=args.n_class, fshape=args.fshape, tshape=args.tshape, fstride=args.fstride, tstride=args.tstride,\n",
    "                       input_fdim=args.num_mel_bins, input_tdim=args.target_length, model_size=args.model_size,\n",
    "                       load_pretrained_mdl_path=args.pretrained_mdl_path)\n",
    "\n",
    "if not isinstance(audio_model, torch.nn.DataParallel):\n",
    "    audio_model = torch.nn.DataParallel(audio_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating experiment directory: %s\" % args.exp_dir)\n",
    "if os.path.exists(\"%s/models\" % args.exp_dir) == False:\n",
    "    os.makedirs(\"%s/models\" % args.exp_dir)\n",
    "with open(\"%s/args.pkl\" % args.exp_dir, \"wb\") as f:\n",
    "    pickle.dump(args, f)\n",
    "\n",
    "if 'pretrain' not in args.task:\n",
    "    print('Now starting fine-tuning for {:d} epochs'.format(args.n_epochs))\n",
    "    train(audio_model, train_loader, val_loader, args)\n",
    "else:\n",
    "    print('Now starting self-supervised pretraining for {:d} epochs'.format(args.n_epochs))\n",
    "    trainmask(audio_model, train_loader, val_loader, args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If fine-tuning, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the dataset has a seperate evaluation set (e.g., speechcommands), then select the model using the validation set and eval on the evaluation set.\n",
    "# this is only for fine-tuning\n",
    "if args.data_eval != None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sd = torch.load(args.exp_dir + '/models/best_audio_model.pth', map_location=device)\n",
    "    if not isinstance(audio_model, torch.nn.DataParallel):\n",
    "        audio_model = torch.nn.DataParallel(audio_model)\n",
    "    audio_model.load_state_dict(sd, strict=False)\n",
    "\n",
    "    # best models on the validation set\n",
    "    args.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    stats, _ = validate(audio_model, val_loader, args, 'valid_set')\n",
    "    # note it is NOT mean of class-wise accuracy\n",
    "    val_acc = stats[0]['acc']\n",
    "    val_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "    print('---------------evaluate on the validation set---------------')\n",
    "    print(\"Accuracy: {:.6f}\".format(val_acc))\n",
    "    print(\"AUC: {:.6f}\".format(val_mAUC))\n",
    "\n",
    "    # test the models on the evaluation set\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudioDataset(args.data_eval, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "    stats, _ = validate(audio_model, eval_loader, args, 'eval_set')\n",
    "    eval_acc = stats[0]['acc']\n",
    "    eval_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "    print('---------------evaluate on the test set---------------')\n",
    "    print(\"Accuracy: {:.6f}\".format(eval_acc))\n",
    "    print(\"AUC: {:.6f}\".format(eval_mAUC))\n",
    "    np.savetxt(args.exp_dir + '/eval_result.csv', [val_acc, val_mAUC, eval_acc, eval_mAUC])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Once you have a model pre-trained to your liking (fine-tuned on your data) you can extract embeddings\n",
    "\n",
    "First prepare the dataframe you would like to get embeddings for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.read_csv(\n",
    "    'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/r01_prelim_161_amrs/test.csv', \n",
    "    index_col = 'uid'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare the bucket where the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = 'r01_prelim'\n",
    "bucket_name = 'ml-e107-phi-shared-aif-us-p'\n",
    "gcs_prefix = f'speech_ai/speech_lake/{study}'\n",
    "\n",
    "storage_client = storage.Client(project=project_name)\n",
    "bucket = storage_client.bucket(bucket_name = bucket_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO get model_name\n",
    "embeddings = get_ssast_embeddings(model_name, args, bucket, gcs_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
