{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "#built-in\n",
    "import argparse\n",
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "#local\n",
    "from dataloader_mayo import AudioDataset\n",
    "from models import ASTModel_pretrain, ASTModel_finetune\n",
    "#from traintest_mayo import train, validate\n",
    "#from traintest_mask_mayo import trainmask\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from google cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'ml-mps-aif-afdgpet01-p-6827'\n",
    "study = 'speech_poc_freeze_1'\n",
    "bucket_name = 'ml-e107-phi-shared-aif-us-p'\n",
    "gcs_prefix = f'speech_ai/speech_lake/{study}'\n",
    "\n",
    "storage_client = storage.Client(project=project_name)\n",
    "bq_client = bigquery.Client(project=project_name)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "file_list=[]\n",
    "for blob in storage_client.list_blobs(bucket_name, prefix=gcs_prefix):\n",
    "    file_list.append(blob.name)\n",
    "\n",
    "    extensions=[f.split('.')[-1] for f in file_list]\n",
    "\n",
    "data_split_root = 'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620'\n",
    "gcs_train_path = f'{data_split_root}/train.csv'\n",
    "gcs_test_path = f'{data_split_root}/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) load the train and test files to a df\n",
    "train_df = pd.read_csv(gcs_train_path, index_col = 'uid')\n",
    "test_df = pd.read_csv(gcs_test_path, index_col = 'uid')\n",
    "\n",
    "# (2) alter columns as necessary \n",
    "train_df[\"distortions\"]=((train_df[\"distorted Cs\"]+train_df[\"distorted V\"])>0).astype(int)\n",
    "test_df[\"distortions\"]=((test_df[\"distorted Cs\"]+test_df[\"distorted V\"])>0).astype(int)\n",
    "\n",
    "# (3) define target labels\n",
    "target_labels=['breathy',\n",
    "             'loudness decay',\n",
    "             'slow rate',\n",
    "             'high pitch',\n",
    "             'hoarse / harsh',\n",
    "             'irregular artic breakdowns',\n",
    "             'rapid rate',\n",
    "             'reduced OA loudness',\n",
    "             'abn pitch variability',\n",
    "             'strained',\n",
    "             'hypernasal',\n",
    "             'abn loudness variability',\n",
    "              'distortions']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set additional variables for running SSAST and set up audio configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio configuration\n",
    "dataset = 'retrospeech'\n",
    "#audio\n",
    "resample_rate = 16000\n",
    "reduce = True\n",
    "clip_length = 0\n",
    "#audio augmentations\n",
    "tshift = 0 #time shift\n",
    "speed = 0\n",
    "gauss = 0 #amt noise\n",
    "pshift = 0 #pitch shift\n",
    "pshiftn = 0 #pitch shift n steps\n",
    "gain = 0\n",
    "stretch = 0\n",
    "#spectrogram\n",
    "dataset_mean = -4.2677393\n",
    "dataset_std = 4.5689974\n",
    "target_length = 1024\n",
    "num_mel_bins = 128\n",
    "freqm = 0\n",
    "timem = 0\n",
    "mixup = 0\n",
    "noise = False\n",
    "#new_audio_conf = {'resample_rate':16000, 'reduce': True, 'clip_length':0, 'tshift':0, 'speed':0, 'gauss_noise':0, 'pshift':0, 'pshiftn':0, 'gain':0, 'stretch': 0, 'num_mel_bins': 128, 'target_length': 1024, 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': 'demo',\n",
    "#              'mode':'train', 'mean':dataset_mean, 'std':dataset_std, 'noise':False}\n",
    "#new_audio_conf = {'resample_rate':16000, 'reduce': True, 'clip_length':0, 'tshift':0.9, 'speed':0, 'gauss_noise':0.8, 'pshift':0, 'pshiftn':0, 'gain':0.9, 'stretch': 0, 'num_mel_bins': 128, 'target_length': 1024, 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': 'demo','mode':'train', 'mean':dataset_mean, 'std':dataset_std, 'noise':False}\n",
    "\n",
    "#train_data = AudioDataset(train_df, target_labels, new_audio_conf, gcs_prefix, bucket)\n",
    "\n",
    "\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(\n",
    " #   train_data,\n",
    "  #  batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "train_audio_conf = {'dataset': dataset, 'mode': 'train', 'resample_rate': resample_rate, 'reduce': reduce, 'clip_length': 0,\n",
    "                    'tshift':tshift, 'speed':speed, 'gauss_noise':gauss, 'pshift':pshift, 'pshiftn':pshiftn, 'gain':gain, 'stretch': stretch,\n",
    "                    'num_mel_bins': num_mel_bins, 'target_length': target_length, 'freqm': freqm, 'timem': timem, 'mixup': mixup, 'noise':noise,\n",
    "                    'mean':dataset_mean, 'std':dataset_std}\n",
    "\n",
    "eval_audio_conf = {'dataset': dataset, 'mode': 'evaluation', 'resample_rate': resample_rate, 'reduce': reduce, 'clip_length': 0,\n",
    "                    'tshift':tshift, 'speed':speed, 'gauss_noise':gauss, 'pshift':pshift, 'pshiftn':pshiftn, 'gain':gain, 'stretch': stretch,\n",
    "                    'num_mel_bins': num_mel_bins, 'target_length': target_length, 'freqm': freqm, 'timem': timem, 'mixup': mixup, 'noise':noise,\n",
    "                    'mean':dataset_mean, 'std':dataset_std}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------the train dataloader---------------\n",
      "now process retrospeech\n",
      "now using following mask: 0 freq, 0 time\n",
      "MIXUP NOT CURRENTLY AVAILABLE\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n",
      "number of classes is 13\n",
      "---------------the evaluation dataloader---------------\n",
      "now process retrospeech\n",
      "now using following mask: 0 freq, 0 time\n",
      "MIXUP NOT CURRENTLY AVAILABLE\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n",
      "number of classes is 13\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AudioDataset(train_df, target_labels, train_audio_conf, gcs_prefix, bucket) #librosa = True (might need to debug this one)\n",
    "\n",
    "test_dataset = AudioDataset(test_df, target_labels, eval_audio_conf, gcs_prefix, bucket)\n",
    "#optional validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNING SSAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False, drop_last=True)\n",
    "\n",
    "#EVENTUALLY ADD IN OPTIONAL VALIDATION\n",
    "eval_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'waveform': tensor([[[ 0.0086,  0.0086,  0.0086,  ..., -0.0047, -0.0044,  0.0023]]]),\n",
       " 'targets': tensor([[0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.]]),\n",
       " 'sample_rate': tensor([16000]),\n",
       " 'fbank': tensor([[[-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],\n",
       "          [-1.2776, -1.2776, -1.2776,  ..., -1.2776, -1.2776, -1.2776],\n",
       "          [-1.2776, -1.2776, -1.2776,  ..., -1.0430, -1.0535, -1.0200],\n",
       "          ...,\n",
       "          [-0.7442, -1.1240, -0.7471,  ..., -0.4353, -0.5055, -0.3359],\n",
       "          [-1.2776, -1.2776, -1.1366,  ..., -0.3679, -0.5114, -0.4016],\n",
       "          [-1.0614, -1.2776, -1.1437,  ..., -0.4080, -0.4991, -0.4675]]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNING THE WAY WE WANT\n",
    "take in a variable saying if we want to freeze the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now load a SSL pretrained models from /Users/m144443/Documents/mayo_ssast/pretrained_model/SSAST-Base-Frame-400.pth\n",
      "pretraining patch split stride: frequency=128, time=2\n",
      "pretraining patch shape: frequency=128, time=2\n",
      "pretraining patch array dimension: frequency=1, time=512\n",
      "pretraining number of patches=512\n",
      "fine-tuning patch split stride: frequncey=128, time=2\n",
      "fine-tuning number of patches=512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m144443/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ast_mdl = ASTModel_finetune(\n",
    "    task='ft_cls', label_dim=len(target_labels), fshape=128, tshape=2, fstride=128, tstride=2, input_fdim=128, input_tdim=target_length, \n",
    "    model_size='base', load_pretrained_mdl_path='/Users/m144443/Documents/mayo_ssast/pretrained_model/SSAST-Base-Frame-400.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 11533\n"
     ]
    }
   ],
   "source": [
    "#FREEZE THE MODEL (only finetuning classifier head)\n",
    "for param in ast_mdl.v.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model_parameters = filter(lambda p: p.requires_grad, ast_mdl.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f'Number of trainable parameters: {params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic running model (no scheduler or anything, no validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.AdamW([p for p in ast_mdl.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6259236772253056\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 #for now just 1 epoch\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        x = batch['fbank']\n",
    "        targets = batch['targets'] #have to change to select targets like this\n",
    "        optim.zero_grad()\n",
    "        o =  ast_mdl(x) #no need for task + give just fbank\n",
    "        loss = criterion(o, targets)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "        print(f'Progress: {round(i/len(train_loader)*100)}%    ',end='\\r')\n",
    "        \n",
    "    print(e, running_loss/len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ast_mdl.state_dict(), 'ast_mdl_base_frame_400_speechfeat_13_adamw_1epoch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99%    \r"
     ]
    }
   ],
   "source": [
    "ast_mdl.eval()\n",
    "all_preds=[]\n",
    "all_targets=[]\n",
    "for i, batch in enumerate(eval_loader):\n",
    "    x = batch['fbank']\n",
    "    targets = batch['targets']\n",
    "    optim.zero_grad()\n",
    "    o=ast_mdl(x)\n",
    "    all_preds.append(o)\n",
    "    all_targets.append(targets)\n",
    "    print(f'Progress: {round(i/len(eval_loader)*100)}%    ',end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39930556 0.51215686 0.60547504 0.38705739 0.5703125  0.53494505\n",
      " 0.57765152 0.66666667 0.44106667 0.47769622 0.27272727 0.42365967\n",
      " 0.57014157]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breathy</td>\n",
       "      <td>0.399306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loudness decay</td>\n",
       "      <td>0.512157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slow rate</td>\n",
       "      <td>0.605475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high pitch</td>\n",
       "      <td>0.387057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hoarse / harsh</td>\n",
       "      <td>0.570312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>irregular artic breakdowns</td>\n",
       "      <td>0.534945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rapid rate</td>\n",
       "      <td>0.577652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reduced OA loudness</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abn pitch variability</td>\n",
       "      <td>0.441067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>strained</td>\n",
       "      <td>0.477696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hypernasal</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abn loudness variability</td>\n",
       "      <td>0.423660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>distortions</td>\n",
       "      <td>0.570142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Label       AUC\n",
       "0                      breathy  0.399306\n",
       "1               loudness decay  0.512157\n",
       "2                    slow rate  0.605475\n",
       "3                   high pitch  0.387057\n",
       "4               hoarse / harsh  0.570312\n",
       "5   irregular artic breakdowns  0.534945\n",
       "6                   rapid rate  0.577652\n",
       "7          reduced OA loudness  0.666667\n",
       "8        abn pitch variability  0.441067\n",
       "9                     strained  0.477696\n",
       "10                  hypernasal  0.272727\n",
       "11    abn loudness variability  0.423660\n",
       "12                 distortions  0.570142"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple metrics\n",
    "pred_mat=torch.sigmoid(torch.cat(all_preds)).detach().numpy()\n",
    "target_mat=torch.cat(all_targets).detach().numpy()\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aucs=roc_auc_score(target_mat, pred_mat, average = None)\n",
    "print(aucs)\n",
    "data = [\n",
    "('Label', target_labels),\n",
    "('AUC', target_labels)]\n",
    "pd.DataFrame({'Label':target_labels, 'AUC':aucs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "sys.path.append(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "from src.utilities import *\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.cuda.amp import autocast,GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(audio_model, train_loader, test_loader, args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('running on ' + str(device))\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    # Initialize all of the statistics we want to keep track of\n",
    "    batch_time = AverageMeter()\n",
    "    per_sample_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    per_sample_data_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    per_sample_dnn_time = AverageMeter()\n",
    "    progress = []\n",
    "    # best_cum_mAP is checkpoint ensemble from the first epoch to the best epoch\n",
    "    best_epoch, best_cum_epoch, best_mAP, best_acc, best_cum_mAP = 0, 0, -np.inf, -np.inf, -np.inf\n",
    "    global_step, epoch = 0, 0\n",
    "    start_time = time.time()\n",
    "    exp_dir = args.exp_dir\n",
    "\n",
    "    def _save_progress():\n",
    "        progress.append([epoch, global_step, best_epoch, best_mAP,\n",
    "                time.time() - start_time])\n",
    "        with open(\"%s/progress.pkl\" % exp_dir, \"wb\") as f:\n",
    "            pickle.dump(progress, f)\n",
    "\n",
    "    if not isinstance(audio_model, nn.DataParallel):\n",
    "        audio_model = nn.DataParallel(audio_model)\n",
    "\n",
    "    audio_model = audio_model.to(device)\n",
    "    # Set up the optimizer\n",
    "    trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
    "    print('Total parameter number is : {:.3f} million'.format(sum(p.numel() for p in audio_model.parameters()) / 1e6))\n",
    "    print('Total trainable parameter number is : {:.3f} million'.format(sum(p.numel() for p in trainables) / 1e6))\n",
    "\n",
    "    # diff lr optimizer\n",
    "    mlp_list = ['mlp_head.0.weight', 'mlp_head.0.bias', 'mlp_head.1.weight', 'mlp_head.1.bias']\n",
    "    mlp_params = list(filter(lambda kv: kv[0] in mlp_list, audio_model.module.named_parameters()))\n",
    "    base_params = list(filter(lambda kv: kv[0] not in mlp_list, audio_model.module.named_parameters()))\n",
    "    mlp_params = [i[1] for i in mlp_params]\n",
    "    base_params = [i[1] for i in base_params]\n",
    "    # only finetuning small/tiny models on balanced audioset uses different learning rate for mlp head\n",
    "    print('The mlp header uses {:d} x larger lr'.format(args.head_lr))\n",
    "    optimizer = torch.optim.Adam([{'params': base_params, 'lr': args.lr}, {'params': mlp_params, 'lr': args.lr * args.head_lr}], weight_decay=5e-7, betas=(0.95, 0.999))\n",
    "    mlp_lr = optimizer.param_groups[1]['lr']\n",
    "    lr_list = [args.lr, mlp_lr]\n",
    "\n",
    "    print('Total mlp parameter number is : {:.3f} million'.format(sum(p.numel() for p in mlp_params) / 1e6))\n",
    "    print('Total base parameter number is : {:.3f} million'.format(sum(p.numel() for p in base_params) / 1e6))\n",
    "\n",
    "    # # dataset specific settings\n",
    "    # if args.dataset == 'audioset':\n",
    "    #     if len(train_loader.dataset) > 2e5:\n",
    "    #         print('scheduler for full audioset is used')\n",
    "    #         scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2,3,4,5], gamma=0.5, last_epoch=-1)\n",
    "    #     else:\n",
    "    #         print('scheduler for balanced audioset is used')\n",
    "    #         scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10, 15, 20, 25], gamma=0.5, last_epoch=-1)\n",
    "    #     main_metrics = 'mAP'\n",
    "    #     loss_fn = nn.BCEWithLogitsLoss()\n",
    "    #     warmup = True\n",
    "    # elif args.dataset == 'esc50':\n",
    "    #     print('scheduler for esc-50 is used')\n",
    "    #     scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(5,26)), gamma=0.85)\n",
    "    #     main_metrics = 'acc'\n",
    "    #     loss_fn = nn.CrossEntropyLoss()\n",
    "    #     warmup = False\n",
    "    # elif args.dataset == 'speechcommands':\n",
    "    #     print('scheduler for speech commands is used')\n",
    "    #     scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(5,26)), gamma=0.85)\n",
    "    #     main_metrics = 'acc'\n",
    "    #     loss_fn = nn.BCEWithLogitsLoss()\n",
    "    #     warmup = False\n",
    "    # else:\n",
    "    #     raise ValueError('unknown dataset, dataset should be in [audioset, speechcommands, esc50]')\n",
    "    # print('now training with {:s}, main metrics: {:s}, loss function: {:s}, learning rate scheduler: {:s}'.format(str(args.dataset), str(main_metrics), str(loss_fn), str(scheduler)))\n",
    "\n",
    "    if args.adaptschedule == True:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=args.lr_patience, verbose=True)\n",
    "        print('now use adaptive learning rate scheduler.')\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(args.lrscheduler_start, 1000, args.lrscheduler_step)),gamma=args.lrscheduler_decay)\n",
    "    main_metrics = args.metrics\n",
    "    if args.loss == 'BCE':\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    elif args.loss == 'CE':\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "    args.loss_fn = loss_fn\n",
    "\n",
    "    print('now training with {:s}, main metrics: {:s}, loss function: {:s}, learning rate scheduler: {:s}'.format(str(args.dataset), str(main_metrics), str(loss_fn), str(scheduler)))\n",
    "    print('The learning rate scheduler starts at {:d} epoch with decay rate of {:.3f} every {:d} epoches'.format(args.lrscheduler_start, args.lrscheduler_decay, args.lrscheduler_step))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
    "    print(\"start training...\")\n",
    "    result = np.zeros([args.n_epochs, 10])\n",
    "    audio_model.train()\n",
    "    while epoch < args.n_epochs + 1:\n",
    "        begin_time = time.time()\n",
    "        end_time = time.time()\n",
    "        audio_model.train()\n",
    "        print('---------------')\n",
    "        print(datetime.datetime.now())\n",
    "        print(\"current #epochs=%s, #steps=%s\" % (epoch, global_step))\n",
    "\n",
    "        for i, (audio_input, labels) in enumerate(train_loader):\n",
    "\n",
    "            B = audio_input.size(0)\n",
    "            audio_input = audio_input.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            data_time.update(time.time() - end_time)\n",
    "            per_sample_data_time.update((time.time() - end_time) / audio_input.shape[0])\n",
    "            dnn_start_time = time.time()\n",
    "\n",
    "            # first several steps for warm-up\n",
    "            if global_step <= 1000 and global_step % 50 == 0 and args.warmup == True:\n",
    "                for group_id, param_group in enumerate(optimizer.param_groups):\n",
    "                    warm_lr = (global_step / 1000) * lr_list[group_id]\n",
    "                    param_group['lr'] = warm_lr\n",
    "                    print('warm-up learning rate is {:f}'.format(param_group['lr']))\n",
    "\n",
    "            audio_output = audio_model(audio_input)\n",
    "            if isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                loss = loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "            else:\n",
    "                loss = loss_fn(audio_output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # record loss\n",
    "            loss_meter.update(loss.item(), B)\n",
    "            batch_time.update(time.time() - end_time)\n",
    "            per_sample_time.update((time.time() - end_time)/audio_input.shape[0])\n",
    "            per_sample_dnn_time.update((time.time() - dnn_start_time)/audio_input.shape[0])\n",
    "\n",
    "            print_step = global_step % args.n_print_steps == 0\n",
    "            early_print_step = epoch == 0 and global_step % (args.n_print_steps/10) == 0\n",
    "            print_step = print_step or early_print_step\n",
    "\n",
    "            if print_step and global_step != 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Per Sample Total Time {per_sample_time.avg:.5f}\\t'\n",
    "                  'Per Sample Data Time {per_sample_data_time.avg:.5f}\\t'\n",
    "                  'Per Sample DNN Time {per_sample_dnn_time.avg:.5f}\\t'\n",
    "                  'Train Loss {loss_meter.avg:.4f}\\t'.format(\n",
    "                   epoch, i, len(train_loader), per_sample_time=per_sample_time, per_sample_data_time=per_sample_data_time,\n",
    "                      per_sample_dnn_time=per_sample_dnn_time, loss_meter=loss_meter), flush=True)\n",
    "                if np.isnan(loss_meter.avg):\n",
    "                    print(\"yuan training diverged...\")\n",
    "                    torch.save(audio_model.state_dict(), \"%s/models/nan_audio_model.pth\" % (exp_dir))\n",
    "                    torch.save(optimizer.state_dict(), \"%s/models/nan_optim_state.pth\" % (exp_dir))\n",
    "                    with open(exp_dir + '/audio_input.npy', 'wb') as f:\n",
    "                        np.save(f, audio_input.cpu().detach().numpy())\n",
    "                    np.savetxt(exp_dir + '/audio_output.csv', audio_output.cpu().detach().numpy(), delimiter=',')\n",
    "                    np.savetxt(exp_dir + '/labels.csv', labels.cpu().detach().numpy(), delimiter=',')\n",
    "                    print('audio output and label saved for debugging.')\n",
    "                    #return\n",
    "\n",
    "            end_time = time.time()\n",
    "            global_step += 1\n",
    "\n",
    "        '''\n",
    "        if not skip_validation:\n",
    "            print('start validation')\n",
    "            stats, valid_loss = validate(audio_model, test_loader, args, epoch)\n",
    "\n",
    "            # ensemble results\n",
    "            cum_stats = validate_ensemble(args, epoch)\n",
    "            cum_mAP = np.mean([stat['AP'] for stat in cum_stats])\n",
    "            cum_mAUC = np.mean([stat['auc'] for stat in cum_stats])\n",
    "            cum_acc = cum_stats[0]['acc']\n",
    "\n",
    "            mAP = np.mean([stat['AP'] for stat in stats])\n",
    "            mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "            acc = stats[0]['acc']\n",
    "\n",
    "            middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
    "            middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
    "            average_precision = np.mean(middle_ps)\n",
    "            average_recall = np.mean(middle_rs)\n",
    "\n",
    "            if main_metrics == 'mAP':\n",
    "                print(\"mAP: {:.6f}\".format(mAP))\n",
    "            else:\n",
    "                print(\"acc: {:.6f}\".format(acc))\n",
    "            print(\"AUC: {:.6f}\".format(mAUC))\n",
    "            print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
    "            print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
    "            print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
    "            print(\"train_loss: {:.6f}\".format(loss_meter.avg))\n",
    "            print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
    "        '''\n",
    "        \n",
    "        if main_metrics == 'mAP':\n",
    "            result[epoch-1, :] = [mAP, mAUC, average_precision, average_recall, d_prime(mAUC), loss_meter.avg, valid_loss, cum_mAP, cum_mAUC, optimizer.param_groups[0]['lr']]\n",
    "        else:\n",
    "            result[epoch-1, :] = [acc, mAUC, average_precision, average_recall, d_prime(mAUC), loss_meter.avg, valid_loss, cum_acc, cum_mAUC, optimizer.param_groups[0]['lr']]\n",
    "        np.savetxt(exp_dir + '/result.csv', result, delimiter=',')\n",
    "        print('validation finished')\n",
    "\n",
    "        if mAP > best_mAP:\n",
    "            best_mAP = mAP\n",
    "            if main_metrics == 'mAP':\n",
    "                best_epoch = epoch\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            if main_metrics == 'acc':\n",
    "                best_epoch = epoch\n",
    "\n",
    "        if cum_mAP > best_cum_mAP:\n",
    "            best_cum_epoch = epoch\n",
    "            best_cum_mAP = cum_mAP\n",
    "\n",
    "        if best_epoch == epoch:\n",
    "            torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
    "            torch.save(optimizer.state_dict(), \"%s/models/best_optim_state.pth\" % (exp_dir))\n",
    "\n",
    "        # save every models\n",
    "        torch.save(audio_model.state_dict(), \"%s/models/audio_model.%d.pth\" % (exp_dir, epoch))\n",
    "        if len(train_loader.dataset) > 2e5:\n",
    "            torch.save(optimizer.state_dict(), \"%s/models/optim_state.%d.pth\" % (exp_dir, epoch))\n",
    "\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            print('adaptive learning rate scheduler step')\n",
    "            scheduler.step(mAP)\n",
    "        else:\n",
    "            print('normal learning rate scheduler step')\n",
    "            scheduler.step()\n",
    "\n",
    "        print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "        print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[1]['lr']))\n",
    "\n",
    "        with open(exp_dir + '/stats_' + str(epoch) +'.pickle', 'wb') as handle:\n",
    "            pickle.dump(stats, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        _save_progress()\n",
    "\n",
    "        finish_time = time.time()\n",
    "        print('epoch {:d} training time: {:.3f}'.format(epoch, finish_time-begin_time))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        # break if lr too small\n",
    "        # if optimizer.param_groups[0]['lr'] < args.lr/64 and epoch > 10:\n",
    "        #     break\n",
    "\n",
    "        batch_time.reset()\n",
    "        per_sample_time.reset()\n",
    "        data_time.reset()\n",
    "        per_sample_data_time.reset()\n",
    "        loss_meter.reset()\n",
    "        per_sample_dnn_time.reset()\n",
    "    '''\n",
    "    if args.wa == True:\n",
    "        stats = validate_wa(audio_model, test_loader, args, args.wa_start, args.wa_end)\n",
    "        mAP = np.mean([stat['AP'] for stat in stats])\n",
    "        mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "        middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
    "        middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
    "        average_precision = np.mean(middle_ps)\n",
    "        average_recall = np.mean(middle_rs)\n",
    "        wa_result = [mAP, mAUC, average_precision, average_recall, d_prime(mAUC)]\n",
    "        print('---------------Training Finished---------------')\n",
    "        print('weighted averaged models results')\n",
    "        print(\"mAP: {:.6f}\".format(mAP))\n",
    "        print(\"AUC: {:.6f}\".format(mAUC))\n",
    "        print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
    "        print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
    "        print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
    "        print(\"train_loss: {:.6f}\".format(loss_meter.avg))\n",
    "        print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
    "        np.savetxt(exp_dir + '/wa_result.csv', wa_result)\n",
    "    '''\n",
    "\n",
    "def validate_ensemble(args, epoch):\n",
    "    exp_dir = args.exp_dir\n",
    "    target = np.loadtxt(exp_dir+'/predictions/target.csv', delimiter=',')\n",
    "    if epoch == 1:\n",
    "        cum_predictions = np.loadtxt(exp_dir + '/predictions/predictions_1.csv', delimiter=',')\n",
    "    else:\n",
    "        cum_predictions = np.loadtxt(exp_dir + '/predictions/cum_predictions.csv', delimiter=',') * (epoch - 1)\n",
    "        predictions = np.loadtxt(exp_dir+'/predictions/predictions_' + str(epoch) + '.csv', delimiter=',')\n",
    "        cum_predictions = cum_predictions + predictions\n",
    "        # remove the prediction file to save storage space\n",
    "        os.remove(exp_dir+'/predictions/predictions_' + str(epoch-1) + '.csv')\n",
    "\n",
    "    cum_predictions = cum_predictions / epoch\n",
    "    np.savetxt(exp_dir+'/predictions/cum_predictions.csv', cum_predictions, delimiter=',')\n",
    "\n",
    "    stats = calculate_stats(cum_predictions, target)\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set arguments for running pre-training/fine-tuning\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--data-train\", type=str, default='train_ssast.json', help=\"training data json\")\n",
    "parser.add_argument(\"--data-val\", type=str, default=None, help=\"validation data json\")\n",
    "parser.add_argument(\"--data-eval\", type=str, default='test_ssast.json', help=\"evaluation data json\")\n",
    "parser.add_argument(\"--label-csv\", type=str, default='./label_df.csv', help=\"csv with class labels\")\n",
    "parser.add_argument(\"--n_class\", type=int, default=len(target_labels), help=\"number of classes\")\n",
    "\n",
    "parser.add_argument(\"--dataset\", type=str, default='demo', help=\"the dataset used for training\")\n",
    "parser.add_argument(\"--dataset_mean\", type=float, default= -4.2677393, help=\"the dataset mean, used for input normalization\")\n",
    "parser.add_argument(\"--dataset_std\", type=float, default=4.5689974, help=\"the dataset std, used for input normalization\")\n",
    "parser.add_argument(\"--target_length\", type=int, default=1024, help=\"the input length in frames\")\n",
    "parser.add_argument(\"--num_mel_bins\", type=int, default=128, help=\"number of input mel bins\")\n",
    "\n",
    "parser.add_argument(\"--exp-dir\", type=str, default=\"\", help=\"directory to dump experiments\")\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.0001, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--warmup', help='if use warmup learning rate scheduler', type=ast.literal_eval, default='True')\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"sgd\", \"adam\"])\n",
    "parser.add_argument('-b', '--batch-size', default=8, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('-w', '--num-workers', default=8, type=int, metavar='NW', help='# of workers for dataloading (default: 32)')\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=80, help=\"number of maximum training epochs\")\n",
    "# only used in pretraining stage or from-scratch fine-tuning experiments\n",
    "parser.add_argument(\"--lr_patience\", type=int, default=2, help=\"how many epoch to wait to reduce lr if mAP doesn't improve\")\n",
    "parser.add_argument('--adaptschedule', help='if use adaptive scheduler ', type=ast.literal_eval, default='False')\n",
    "\n",
    "parser.add_argument(\"--n-print-steps\", type=int, default=100, help=\"number of steps to print statistics\")\n",
    "parser.add_argument('--save_model', help='save the models or not', type=ast.literal_eval, default='True')\n",
    "\n",
    "parser.add_argument('--freqm', help='frequency mask max length', type=int, default=0)\n",
    "parser.add_argument('--timem', help='time mask max length', type=int, default=0)\n",
    "parser.add_argument(\"--mixup\", type=float, default=0, help=\"how many (0-1) samples need to be mixup during training\")\n",
    "parser.add_argument(\"--bal\", type=str, default=None, help=\"use balanced sampling or not\")\n",
    "# the stride used in patch spliting, e.g., for patch size 16*16, a stride of 16 means no overlapping, a stride of 10 means overlap of 6.\n",
    "# during self-supervised pretraining stage, no patch split overlapping is used (to aviod shortcuts), i.e., fstride=fshape and tstride=tshape\n",
    "# during fine-tuning, using patch split overlapping (i.e., smaller {f,t}stride than {f,t}shape) improves the performance.\n",
    "# it is OK to use different {f,t} stride in pretraining and finetuning stages (though fstride is better to keep the same)\n",
    "# but {f,t}stride in pretraining and finetuning stages must be consistent.\n",
    "parser.add_argument(\"--fstride\", type=int,default=128,help=\"soft split freq stride, overlap=patch_size-stride\")\n",
    "parser.add_argument(\"--tstride\", type=int,default=2, help=\"soft split time stride, overlap=patch_size-stride\")\n",
    "parser.add_argument(\"--fshape\", type=int, defaut=128, help=\"shape of patch on the frequency dimension\")\n",
    "parser.add_argument(\"--tshape\", type=int, default=2, help=\"shape of patch on the time dimension\")\n",
    "parser.add_argument('--model_size', help='the size of AST models', type=str, default='base')\n",
    "\n",
    "parser.add_argument(\"--task\", type=str, default='ft_cls', help=\"pretraining or fine-tuning task\", choices=[\"ft_avgtok\", \"ft_cls\", \"pretrain_mpc\", \"pretrain_mpg\", \"pretrain_joint\"])\n",
    "\n",
    "# pretraining augments\n",
    "#parser.add_argument('--pretrain_stage', help='True for self-supervised pretraining stage, False for fine-tuning stage', type=ast.literal_eval, default='False')\n",
    "parser.add_argument('--mask_patch', help='how many patches to mask (used only for ssl pretraining)', type=int, default=400)\n",
    "parser.add_argument(\"--cluster_factor\", type=int, default=3, help=\"mask clutering factor\")\n",
    "parser.add_argument(\"--epoch_iter\", type=int, default=2000, help=\"for pretraining, how many iterations to verify and save models\")\n",
    "\n",
    "# fine-tuning arguments\n",
    "parser.add_argument(\"--pretrained_mdl_path\", type=str, default='./pretrained_model/SSAST-Base-Frame-400.pth', help=\"the ssl pretrained models path\")\n",
    "parser.add_argument(\"--head_lr\", type=int, default=1, help=\"the factor of mlp-head_lr/lr, used in some fine-tuning experiments only\")\n",
    "parser.add_argument(\"--noise\", help='if augment noise in finetuning', type=ast.literal_eval, default='False')\n",
    "parser.add_argument(\"--metrics\", type=str, default=\"mAP\", help=\"the main evaluation metrics in finetuning\", choices=[\"mAP\", \"acc\"])\n",
    "parser.add_argument(\"--lrscheduler_start\", default=10, type=int, help=\"when to start decay in finetuning\")\n",
    "parser.add_argument(\"--lrscheduler_step\", default=5, type=int, help=\"the number of step to decrease the learning rate in finetuning\")\n",
    "parser.add_argument(\"--lrscheduler_decay\", default=0.5, type=float, help=\"the learning rate decay ratio in finetuning\")\n",
    "parser.add_argument(\"--wa\", help='if do weight averaging in finetuning', type=ast.literal_eval, default='False')\n",
    "parser.add_argument(\"--wa_start\", type=int, default=16, help=\"which epoch to start weight averaging in finetuning\")\n",
    "parser.add_argument(\"--wa_end\", type=int, default=30, help=\"which epoch to end weight averaging in finetuning\")\n",
    "parser.add_argument(\"--loss\", type=str, default=\"BCE\", help=\"the loss function for finetuning, depend on the task\", choices=[\"BCE\", \"CE\"])\n",
    "\n",
    "args = parser.parse_args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
