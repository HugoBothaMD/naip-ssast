{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSAST for speech\n",
    "All functions copied from this repo and then edited as indicated\n",
    "\n",
    "https://github.com/YuanGongND/ssast\n",
    "Additional authors: Matt, Daniela\n",
    "\n",
    "Original ASTModel class was split into two - one for pretraining and one for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, you will need access to google cloud storage bucket and the following packages must be installed on your system \n",
    "\n",
    "* opencv-python\n",
    "* albumentations (may run into issues in AIF)\n",
    "* librosa\n",
    "* torch, torchvision, torchaudio\n",
    "\n",
    "(can ignore the following if using AIF)\n",
    "* google-cloud\n",
    "* google-cloud-storage\n",
    "* google-cloud-bigquery\n",
    "\n",
    "If working on a local computer, you can run the following commands to gain access to the google storage bucket\n",
    "\n",
    "```gcloud auth application-default login```\n",
    "```gcloud auth application-defaul set-quota-project PROJECT_NAME```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from google.cloud import storage, bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/ssast_mayo/ssast/src'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ast_models import ASTModel_pretrain, ASTModel_finetune\n",
    "from dataloader_gcs import AudioDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.ssast_utils import *\n",
    "from utilities.speech_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load data from google storage bucket\n",
    "\n",
    "project_name = 'ml-mps-aif-afdgpet01-p-6827'\n",
    "study = 'speech_poc_freeze_1'\n",
    "bucket_name = 'ml-e107-phi-shared-aif-us-p'\n",
    "gcs_prefix = f'speech_ai/speech_lake/{study}'\n",
    "\n",
    "storage_client = storage.Client(project=project_name)\n",
    "bq_client = bigquery.Client(project=project_name)\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "file_list=[]\n",
    "for blob in storage_client.list_blobs(bucket_name, prefix='speech_ai/speech_lake/speech_poc_freeze_1'):\n",
    "    file_list.append(blob.name)\n",
    "\n",
    "    extensions=[f.split('.')[-1] for f in file_list]\n",
    "\n",
    "data_split_root = 'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/amr_subject_dedup_594_train_100_test_binarized_v20220620'\n",
    "gcs_train_path = f'{data_split_root}/train.csv'\n",
    "gcs_test_path = f'{data_split_root}/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (1) load the train and test files to a df\n",
    "train_df = pd.read_csv(gcs_train_path, index_col = 'uid')\n",
    "test_df = pd.read_csv(gcs_test_path, index_col = 'uid')\n",
    "\n",
    "# (2) alter columns as necessary \n",
    "train_df[\"distortions\"]=((train_df[\"distorted Cs\"]+train_df[\"distorted V\"])>0).astype(int)\n",
    "test_df[\"distortions\"]=((test_df[\"distorted Cs\"]+test_df[\"distorted V\"])>0).astype(int)\n",
    "\n",
    "# (3) define target labels\n",
    "target_labels=['breathy',\n",
    "             'loudness decay',\n",
    "             'slow rate',\n",
    "             'high pitch',\n",
    "             'hoarse / harsh',\n",
    "             'irregular artic breakdowns',\n",
    "             'rapid rate',\n",
    "             'reduced OA loudness',\n",
    "             'abn pitch variability',\n",
    "             'strained',\n",
    "             'hypernasal',\n",
    "             'abn loudness variability',\n",
    "              'distortions']\n",
    "\n",
    "# (4) select only the target labels from train and test df\n",
    "train_df=train_df[target_labels]\n",
    "test_df=train_df[target_labels]\n",
    "\n",
    "# (5) prep the data\n",
    "prep_ssast_data(train_df,target_labels,'train_ssast',create_label_csv=True)\n",
    "prep_ssast_data(test_df,target_labels,'test_ssast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breathy</th>\n",
       "      <th>loudness decay</th>\n",
       "      <th>slow rate</th>\n",
       "      <th>high pitch</th>\n",
       "      <th>hoarse / harsh</th>\n",
       "      <th>irregular artic breakdowns</th>\n",
       "      <th>rapid rate</th>\n",
       "      <th>reduced OA loudness</th>\n",
       "      <th>abn pitch variability</th>\n",
       "      <th>strained</th>\n",
       "      <th>hypernasal</th>\n",
       "      <th>abn loudness variability</th>\n",
       "      <th>distortions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4d28f730-5814-48e1-bc29-3c0bf562e2fb</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f31c13e4-9f49-411e-b59f-f692244fb740</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d917de91-c421-40bf-9d75-a0b5b0736c5b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9c4a9e77-3080-4591-8797-d712e42d6ed6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      breathy  loudness decay  slow rate  \\\n",
       "uid                                                                        \n",
       "4d28f730-5814-48e1-bc29-3c0bf562e2fb        0               0          1   \n",
       "1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4        0               0          1   \n",
       "f31c13e4-9f49-411e-b59f-f692244fb740        1               1          0   \n",
       "d917de91-c421-40bf-9d75-a0b5b0736c5b        0               0          0   \n",
       "9c4a9e77-3080-4591-8797-d712e42d6ed6        1               0          0   \n",
       "\n",
       "                                      high pitch  hoarse / harsh  \\\n",
       "uid                                                                \n",
       "4d28f730-5814-48e1-bc29-3c0bf562e2fb           0               1   \n",
       "1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4           0               0   \n",
       "f31c13e4-9f49-411e-b59f-f692244fb740           0               1   \n",
       "d917de91-c421-40bf-9d75-a0b5b0736c5b           0               0   \n",
       "9c4a9e77-3080-4591-8797-d712e42d6ed6           0               1   \n",
       "\n",
       "                                      irregular artic breakdowns  rapid rate  \\\n",
       "uid                                                                            \n",
       "4d28f730-5814-48e1-bc29-3c0bf562e2fb                           0           0   \n",
       "1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4                           0           0   \n",
       "f31c13e4-9f49-411e-b59f-f692244fb740                           0           0   \n",
       "d917de91-c421-40bf-9d75-a0b5b0736c5b                           0           1   \n",
       "9c4a9e77-3080-4591-8797-d712e42d6ed6                           0           0   \n",
       "\n",
       "                                      reduced OA loudness  \\\n",
       "uid                                                         \n",
       "4d28f730-5814-48e1-bc29-3c0bf562e2fb                    0   \n",
       "1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4                    0   \n",
       "f31c13e4-9f49-411e-b59f-f692244fb740                    0   \n",
       "d917de91-c421-40bf-9d75-a0b5b0736c5b                    1   \n",
       "9c4a9e77-3080-4591-8797-d712e42d6ed6                    0   \n",
       "\n",
       "                                      abn pitch variability  strained  \\\n",
       "uid                                                                     \n",
       "4d28f730-5814-48e1-bc29-3c0bf562e2fb                      0         1   \n",
       "1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4                      1         0   \n",
       "f31c13e4-9f49-411e-b59f-f692244fb740                      0         1   \n",
       "d917de91-c421-40bf-9d75-a0b5b0736c5b                      0         0   \n",
       "9c4a9e77-3080-4591-8797-d712e42d6ed6                      0         1   \n",
       "\n",
       "                                      hypernasal  abn loudness variability  \\\n",
       "uid                                                                          \n",
       "4d28f730-5814-48e1-bc29-3c0bf562e2fb           0                         0   \n",
       "1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4           0                         1   \n",
       "f31c13e4-9f49-411e-b59f-f692244fb740           1                         0   \n",
       "d917de91-c421-40bf-9d75-a0b5b0736c5b           0                         0   \n",
       "9c4a9e77-3080-4591-8797-d712e42d6ed6           0                         0   \n",
       "\n",
       "                                      distortions  \n",
       "uid                                                \n",
       "4d28f730-5814-48e1-bc29-3c0bf562e2fb            0  \n",
       "1e2dedd0-4f93-42ee-b0fb-c77fb7ba4cf4            0  \n",
       "f31c13e4-9f49-411e-b59f-f692244fb740            1  \n",
       "d917de91-c421-40bf-9d75-a0b5b0736c5b            1  \n",
       "9c4a9e77-3080-4591-8797-d712e42d6ed6            1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SSAST\n",
    "additional imports to support running SSAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "basepath = os.path.dirname(os.path.dirname(sys.path[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/ssast_mayo'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(basepath)\n",
    "import dataloader_gcs as dataloader\n",
    "from models.ast_models import ASTModel_pretrain, ASTModel_finetune\n",
    "import numpy as np\n",
    "from traintest import train, validate\n",
    "from traintest_mask import trainmask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set arguments for running SSAST\n",
    "#TODO: label csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set arguments for running pre-training/fine-tuning\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data-train\", type=str, default='train_ssast.json', help=\"training data json\")\n",
    "parser.add_argument(\"--data-val\", type=str, default='test_ssast.json', help=\"validation data json\")\n",
    "parser.add_argument(\"--data-eval\", type=str, default=None, help=\"evaluation data json\")\n",
    "parser.add_argument(\"--label-csv\", type=str, default='label_df.csv', help=\"csv with class labels\")\n",
    "parser.add_argument(\"--n_class\", type=int, default=len(target_labels), help=\"number of classes\")\n",
    "\n",
    "parser.add_argument(\"--dataset\", type=str, default='demo', help=\"the dataset used for training\")\n",
    "parser.add_argument(\"--dataset_mean\", type=float, default= -4.2677393, help=\"the dataset mean, used for input normalization\")\n",
    "parser.add_argument(\"--dataset_std\", type=float, default=4.5689974, help=\"the dataset std, used for input normalization\")\n",
    "parser.add_argument(\"--target_length\", type=int, default=1024, help=\"the input length in frames\")\n",
    "parser.add_argument(\"--num_mel_bins\", type=int, default=128, help=\"number of input mel bins\")\n",
    "\n",
    "parser.add_argument(\"--exp-dir\", type=str, default=\"experiments\", help=\"directory to dump experiments\")\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.0001, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--warmup', help='if use warmup learning rate scheduler', type=ast.literal_eval, default='True')\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"sgd\", \"adam\"])\n",
    "parser.add_argument('-b', '--batch-size', default=8, type=int, metavar='N', help='mini-batch size')\n",
    "parser.add_argument('-w', '--num-workers', default=8, type=int, metavar='NW', help='# of workers for dataloading (default: 32)')\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=80, help=\"number of maximum training epochs\")\n",
    "# only used in pretraining stage or from-scratch fine-tuning experiments\n",
    "parser.add_argument(\"--lr_patience\", type=int, default=2, help=\"how many epoch to wait to reduce lr if mAP doesn't improve\")\n",
    "parser.add_argument('--adaptschedule', help='if use adaptive scheduler ', type=ast.literal_eval, default='False')\n",
    "\n",
    "parser.add_argument(\"--n-print-steps\", type=int, default=100, help=\"number of steps to print statistics\")\n",
    "parser.add_argument('--save_model', help='save the models or not', type=ast.literal_eval, default='True')\n",
    "\n",
    "parser.add_argument('--freqm', help='frequency mask max length', type=int, default=0)\n",
    "parser.add_argument('--timem', help='time mask max length', type=int, default=0)\n",
    "parser.add_argument(\"--mixup\", type=float, default=0, help=\"how many (0-1) samples need to be mixup during training\")\n",
    "parser.add_argument(\"--bal\", type=str, default=None, help=\"use balanced sampling or not\")\n",
    "# the stride used in patch spliting, e.g., for patch size 16*16, a stride of 16 means no overlapping, a stride of 10 means overlap of 6.\n",
    "# during self-supervised pretraining stage, no patch split overlapping is used (to aviod shortcuts), i.e., fstride=fshape and tstride=tshape\n",
    "# during fine-tuning, using patch split overlapping (i.e., smaller {f,t}stride than {f,t}shape) improves the performance.\n",
    "# it is OK to use different {f,t} stride in pretraining and finetuning stages (though fstride is better to keep the same)\n",
    "# but {f,t}stride in pretraining and finetuning stages must be consistent.\n",
    "parser.add_argument(\"--fstride\", type=int,default=128,help=\"soft split freq stride, overlap=patch_size-stride\")\n",
    "parser.add_argument(\"--tstride\", type=int,default=2, help=\"soft split time stride, overlap=patch_size-stride\")\n",
    "parser.add_argument(\"--fshape\", type=int, default=128, help=\"shape of patch on the frequency dimension\")\n",
    "parser.add_argument(\"--tshape\", type=int, default=2, help=\"shape of patch on the time dimension\")\n",
    "parser.add_argument('--model_size', help='the size of AST models', type=str, default='base')\n",
    "\n",
    "parser.add_argument(\"--task\", type=str, default='ft_cls', help=\"pretraining or fine-tuning task\", choices=[\"ft_avgtok\", \"ft_cls\", \"pretrain_mpc\", \"pretrain_mpg\", \"pretrain_joint\"])\n",
    "\n",
    "# pretraining augments\n",
    "#parser.add_argument('--pretrain_stage', help='True for self-supervised pretraining stage, False for fine-tuning stage', type=ast.literal_eval, default='False')\n",
    "parser.add_argument('--mask_patch', help='how many patches to mask (used only for ssl pretraining)', type=int, default=400)\n",
    "parser.add_argument(\"--cluster_factor\", type=int, default=3, help=\"mask clutering factor\")\n",
    "parser.add_argument(\"--epoch_iter\", type=int, default=2000, help=\"for pretraining, how many iterations to verify and save models\")\n",
    "\n",
    "# fine-tuning arguments\n",
    "parser.add_argument(\"--pretrained_mdl_path\", type=str, default='SSAST-Base-Frame-400.pth', help=\"the ssl pretrained models path\")\n",
    "parser.add_argument(\"--head_lr\", type=int, default=1, help=\"the factor of mlp-head_lr/lr, used in some fine-tuning experiments only\")\n",
    "parser.add_argument(\"--noise\", help='if augment noise in finetuning', type=ast.literal_eval, default='False')\n",
    "parser.add_argument(\"--metrics\", type=str, default=\"mAP\", help=\"the main evaluation metrics in finetuning\", choices=[\"mAP\", \"acc\"])\n",
    "parser.add_argument(\"--lrscheduler_start\", default=10, type=int, help=\"when to start decay in finetuning\")\n",
    "parser.add_argument(\"--lrscheduler_step\", default=5, type=int, help=\"the number of step to decrease the learning rate in finetuning\")\n",
    "parser.add_argument(\"--lrscheduler_decay\", default=0.5, type=float, help=\"the learning rate decay ratio in finetuning\")\n",
    "parser.add_argument(\"--wa\", help='if do weight averaging in finetuning', type=ast.literal_eval, default='False')\n",
    "parser.add_argument(\"--wa_start\", type=int, default=16, help=\"which epoch to start weight averaging in finetuning\")\n",
    "parser.add_argument(\"--wa_end\", type=int, default=30, help=\"which epoch to end weight averaging in finetuning\")\n",
    "parser.add_argument(\"--loss\", type=str, default=\"BCE\", help=\"the loss function for finetuning, depend on the task\", choices=[\"BCE\", \"CE\"])\n",
    "\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset spectrogram mean and std, used to normalize the input\n",
    "# norm_stats = {'librispeech':[-4.2677393, 4.5689974], 'howto100m':[-4.2677393, 4.5689974], 'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
    "# target_length = {'librispeech': 1024, 'howto100m':1024, 'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "# # if add noise for data augmentation, only use for speech commands\n",
    "# noise = {'librispeech': False, 'howto100m': False, 'audioset': False, 'esc50': False, 'speechcommands':True}\n",
    "\n",
    "audio_conf = {'num_mel_bins': args.num_mel_bins, 'target_length': args.target_length, 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup, 'dataset': args.dataset,\n",
    "              'mode':'train', 'mean':args.dataset_mean, 'std':args.dataset_std, 'noise':args.noise}\n",
    "\n",
    "val_audio_conf = {'num_mel_bins': args.num_mel_bins, 'target_length': args.target_length, 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': args.dataset, 'mode': 'evaluation', 'mean': args.dataset_mean, 'std': args.dataset_std, 'noise': False}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------the train dataloader---------------\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process demo\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n",
      "number of classes is 289\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataloader.AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, bucket=bucket, gcs_prefix=gcs_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load audio from <_io.BytesIO object at 0x7fa42ad2a950>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5388/2181952334.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ssast_mayo/ssast/src/dataloader_gcs.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mdatum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mlabel_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mfbank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmix_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wav2fbank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mlabel_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ssast_mayo/ssast/src/dataloader_gcs.py\u001b[0m in \u001b[0;36m_wav2fbank\u001b[0;34m(self, filename, filename2)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_waveform_from_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcs_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_rate_hz'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ssast_mayo/ssast/src/dataloader_gcs.py\u001b[0m in \u001b[0;36mload_waveform_from_gcs\u001b[0;34m(bucket, gcs_prefix, uid, extension)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mgcs_metadata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{gcs_prefix}/{uid}/metadata.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwave_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mmetadata_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcs_metadata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# For the special BC for mp3, we handle mp3 differently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mp3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_fallback_load_fileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             ret = torchaudio._torchaudio.load_audio_fileobj(\n\u001b[1;32m    216\u001b[0m                 \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36m_fail_load_fileobj\u001b[0;34m(fileobj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fail_load_fileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio from {fileobj}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load audio from <_io.BytesIO object at 0x7fa42ad2a950>"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced sampler is not used\n",
      "---------------the train dataloader---------------\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process demo\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n",
      "number of classes is 289\n",
      "---------------the evaluation dataloader---------------\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process demo\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n",
      "number of classes is 289\n"
     ]
    }
   ],
   "source": [
    "# if use balanced sampling, note - self-supervised pretraining should not use balance sampling as it implicitly leverages the label information.\n",
    "if args.bal == 'bal':\n",
    "    print('balanced sampler is being used')\n",
    "    samples_weight = np.loadtxt(args.data_train[:-5]+'_weight.csv', delimiter=',')\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, bucket=bucket, gcs_prefix=gcs_prefix),\n",
    "        batch_size=args.batch_size, sampler=sampler, num_workers=args.num_workers, pin_memory=False, drop_last=True)\n",
    "else:\n",
    "    print('balanced sampler is not used')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudioDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, bucket=bucket, gcs_prefix=gcs_prefix),\n",
    "        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=False, drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataloader.AudioDataset(args.data_val, label_csv=args.label_csv, audio_conf=val_audio_conf,bucket=bucket, gcs_prefix=gcs_prefix),\n",
    "    batch_size=args.batch_size * 2, shuffle=False, num_workers=args.num_workers, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader_gcs.py\", line 435, in __getitem__\n    fbank, mix_lambda = self._wav2fbank(datum['wav'])\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader_gcs.py\", line 315, in _wav2fbank\n    waveform, metadata = load_waveform_from_gcs(self.bucket,self.gcs_prefix,filename)\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader_gcs.py\", line 205, in load_waveform_from_gcs\n    waveform, _ = torchaudio.load(wave_bytes, format = extension)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 214, in load\n    return _fallback_load_fileobj(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 33, in _fail_load_fileobj\n    raise RuntimeError(f\"Failed to load audio from {fileobj}\")\nRuntimeError: Failed to load audio from <_io.BytesIO object at 0x7fa42994e8f0>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5388/4187007541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader_gcs.py\", line 435, in __getitem__\n    fbank, mix_lambda = self._wav2fbank(datum['wav'])\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader_gcs.py\", line 315, in _wav2fbank\n    waveform, metadata = load_waveform_from_gcs(self.bucket,self.gcs_prefix,filename)\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader_gcs.py\", line 205, in load_waveform_from_gcs\n    waveform, _ = torchaudio.load(wave_bytes, format = extension)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 214, in load\n    return _fallback_load_fileobj(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 33, in _fail_load_fileobj\n    raise RuntimeError(f\"Failed to load audio from {fileobj}\")\nRuntimeError: Failed to load audio from <_io.BytesIO object at 0x7fa42994e8f0>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now train with demo with 594 training samples, evaluate with 594 samples\n",
      "now load a SSL pretrained models from SSAST-Base-Frame-400.pth\n",
      "pretraining patch split stride: frequency=128, time=2\n",
      "pretraining patch shape: frequency=128, time=2\n",
      "pretraining patch array dimension: frequency=1, time=512\n",
      "pretraining number of patches=512\n",
      "fine-tuning patch split stride: frequncey=128, time=2\n",
      "fine-tuning number of patches=512\n"
     ]
    }
   ],
   "source": [
    "print('Now train with {:s} with {:d} training samples, evaluate with {:d} samples'.format(args.dataset, len(train_loader.dataset), len(val_loader.dataset)))\n",
    "\n",
    "# in the pretraining stage\n",
    "if 'pretrain' in args.task:\n",
    "    cluster = (args.num_mel_bins != args.fshape)\n",
    "    if cluster == True:\n",
    "        print('The num_mel_bins {:d} and fshape {:d} are different, not masking a typical time frame, using cluster masking.'.format(args.num_mel_bins, args.fshape))\n",
    "    else:\n",
    "        print('The num_mel_bins {:d} and fshape {:d} are same, masking a typical time frame, not using cluster masking.'.format(args.num_mel_bins, args.fshape))\n",
    "    # no label dimension needed as it is self-supervised, fshape=fstride and tshape=tstride\n",
    "    audio_model = ASTModel_pretrain(fshape=args.fshape, tshape=args.tshape, fstride=args.fshape, tstride=args.tshape,\n",
    "                       input_fdim=args.num_mel_bins, input_tdim=args.target_length, model_size=args.model_size, load_pretrained_mdl_path=args.pretrained_mdl_pth)\n",
    "# in the fine-tuning stage\n",
    "else:\n",
    "    audio_model = ASTModel_finetune(task = args.task, label_dim=args.n_class, fshape=args.fshape, tshape=args.tshape, fstride=args.fstride, tstride=args.tstride,\n",
    "                       input_fdim=args.num_mel_bins, input_tdim=args.target_length, model_size=args.model_size,\n",
    "                       load_pretrained_mdl_path=args.pretrained_mdl_path)\n",
    "\n",
    "if not isinstance(audio_model, torch.nn.DataParallel):\n",
    "    audio_model = torch.nn.DataParallel(audio_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating experiment directory: experiments\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating experiment directory: %s\" % args.exp_dir)\n",
    "if os.path.exists(\"%s/models\" % args.exp_dir) == False:\n",
    "    os.makedirs(\"%s/models\" % args.exp_dir)\n",
    "with open(\"%s/args.pkl\" % args.exp_dir, \"wb\") as f:\n",
    "    pickle.dump(args, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 177, in __getitem__\n    fbank, mix_lambda = self._wav2fbank(datum['wav'])\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 98, in _wav2fbank\n    waveform, sr = torchaudio.load(filename)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 227, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 29, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from ce8c1402-8f50-4580-92ed-48e67b0fa756\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5388/4187007541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 177, in __getitem__\n    fbank, mix_lambda = self._wav2fbank(datum['wav'])\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 98, in _wav2fbank\n    waveform, sr = torchaudio.load(filename)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 227, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 29, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from ce8c1402-8f50-4580-92ed-48e67b0fa756\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now starting fine-tuning for 80 epochs\n",
      "running on cpu\n",
      "Total parameter number is : 87.199 million\n",
      "Total trainable parameter number is : 87.199 million\n",
      "The mlp header uses 1 x larger lr\n",
      "Total mlp parameter number is : 0.012 million\n",
      "Total base parameter number is : 87.188 million\n",
      "now training with demo, main metrics: mAP, loss function: BCEWithLogitsLoss(), learning rate scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x7fa42aeb2750>\n",
      "The learning rate scheduler starts at 10 epoch with decay rate of 0.500 every 5 epoches\n",
      "current #steps=0, #epochs=1\n",
      "start training...\n",
      "---------------\n",
      "2023-04-20 15:34:40.012710\n",
      "current #epochs=1, #steps=0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 177, in __getitem__\n    fbank, mix_lambda = self._wav2fbank(datum['wav'])\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 98, in _wav2fbank\n    waveform, sr = torchaudio.load(filename)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 227, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 29, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from 6822ad12-e1ae-4e3c-ba66-1ffe43401244\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5388/2195084101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'pretrain'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Now starting fine-tuning for {:d} epochs'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Now starting self-supervised pretraining for {:d} epochs'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ssast_mayo/ssast/src/traintest.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(audio_model, train_loader, test_loader, args)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current #epochs=%s, #steps=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maudio_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 177, in __getitem__\n    fbank, mix_lambda = self._wav2fbank(datum['wav'])\n  File \"/home/jupyter/ssast_mayo/ssast/src/dataloader.py\", line 98, in _wav2fbank\n    waveform, sr = torchaudio.load(filename)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 227, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/sox_io_backend.py\", line 29, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from 6822ad12-e1ae-4e3c-ba66-1ffe43401244\n"
     ]
    }
   ],
   "source": [
    "if 'pretrain' not in args.task:\n",
    "    print('Now starting fine-tuning for {:d} epochs'.format(args.n_epochs))\n",
    "    train(audio_model, train_loader, val_loader, args)\n",
    "else:\n",
    "    print('Now starting self-supervised pretraining for {:d} epochs'.format(args.n_epochs))\n",
    "    trainmask(audio_model, train_loader, val_loader, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If fine-tuning, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the dataset has a seperate evaluation set (e.g., speechcommands), then select the model using the validation set and eval on the evaluation set.\n",
    "# this is only for fine-tuning\n",
    "if args.data_eval != None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sd = torch.load(args.exp_dir + '/models/best_audio_model.pth', map_location=device)\n",
    "    if not isinstance(audio_model, torch.nn.DataParallel):\n",
    "        audio_model = torch.nn.DataParallel(audio_model)\n",
    "    audio_model.load_state_dict(sd, strict=False)\n",
    "\n",
    "    # best models on the validation set\n",
    "    args.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    stats, _ = validate(audio_model, val_loader, args, 'valid_set')\n",
    "    # note it is NOT mean of class-wise accuracy\n",
    "    val_acc = stats[0]['acc']\n",
    "    val_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "    print('---------------evaluate on the validation set---------------')\n",
    "    print(\"Accuracy: {:.6f}\".format(val_acc))\n",
    "    print(\"AUC: {:.6f}\".format(val_mAUC))\n",
    "\n",
    "    # test the models on the evaluation set\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudioDataset(args.data_eval, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "    stats, _ = validate(audio_model, eval_loader, args, 'eval_set')\n",
    "    eval_acc = stats[0]['acc']\n",
    "    eval_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "    print('---------------evaluate on the test set---------------')\n",
    "    print(\"Accuracy: {:.6f}\".format(eval_acc))\n",
    "    print(\"AUC: {:.6f}\".format(eval_mAUC))\n",
    "    np.savetxt(args.exp_dir + '/eval_result.csv', [val_acc, val_mAUC, eval_acc, eval_mAUC])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Once you have a model pre-trained to your liking (fine-tuned on your data) you can extract embeddings\n",
    "\n",
    "First prepare the dataframe you would like to get embeddings for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.read_csv(\n",
    "    'gs://ml-e107-phi-shared-aif-us-p/speech_ai/share/data_splits/r01_prelim_161_amrs/test.csv', \n",
    "    index_col = 'uid'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare the bucket where the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = 'r01_prelim'\n",
    "bucket_name = 'ml-e107-phi-shared-aif-us-p'\n",
    "gcs_prefix = f'speech_ai/speech_lake/{study}'\n",
    "\n",
    "storage_client = storage.Client(project=project_name)\n",
    "bucket = storage_client.bucket(bucket_name = bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO get model_name\n",
    "embeddings = get_ssast_embeddings(model_name, args, bucket, gcs_prefix)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
